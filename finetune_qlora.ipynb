{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-29T02:50:55.969636Z","iopub.execute_input":"2023-10-29T02:50:55.970014Z","iopub.status.idle":"2023-10-29T02:50:58.066251Z","shell.execute_reply.started":"2023-10-29T02:50:55.969979Z","shell.execute_reply":"2023-10-29T02:50:58.065403Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:40:18.516744Z","iopub.execute_input":"2023-10-29T03:40:18.517127Z","iopub.status.idle":"2023-10-29T03:40:18.522030Z","shell.execute_reply.started":"2023-10-29T03:40:18.517098Z","shell.execute_reply":"2023-10-29T03:40:18.520907Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\").to('cuda')\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")","metadata":{"execution":{"iopub.status.busy":"2023-10-29T02:51:01.943577Z","iopub.execute_input":"2023-10-29T02:51:01.944074Z","iopub.status.idle":"2023-10-29T02:51:42.882549Z","shell.execute_reply.started":"2023-10-29T02:51:01.944045Z","shell.execute_reply":"2023-10-29T02:51:42.881706Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cd567e56071432896f6439683580f79"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8c531ac7c54d77a9b7b54fbc8f2114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c36aeb69736e4b46953b482cae9168a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf7d274687f04810b9beac2124ce9691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4550b8397de4db18454c04f2e23e904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28707ff1490f4efe9ec6bb8cda7db6b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fef5571fc524bc6af395b241c5602ac"}},"metadata":{}}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/megathon-context-qna/train_context.csv')\nqna = pd.read_csv('/kaggle/input/megathon-context-qna/train_qna.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-29T02:52:40.863906Z","iopub.execute_input":"2023-10-29T02:52:40.864656Z","iopub.status.idle":"2023-10-29T02:52:41.234529Z","shell.execute_reply.started":"2023-10-29T02:52:40.864623Z","shell.execute_reply":"2023-10-29T02:52:41.233612Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(data, qna, left_on='index', right_on='context', how='inner')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T02:52:42.366592Z","iopub.execute_input":"2023-10-29T02:52:42.367289Z","iopub.status.idle":"2023-10-29T02:52:42.406321Z","shell.execute_reply.started":"2023-10-29T02:52:42.367260Z","shell.execute_reply":"2023-10-29T02:52:42.405174Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   index                                          context_x  context_y  \\\n0      0   Gastritis is an inflammation, irritation, or ...          0   \n1      0   Gastritis is an inflammation, irritation, or ...          0   \n2      0   Gastritis is an inflammation, irritation, or ...          0   \n3      0   Gastritis is an inflammation, irritation, or ...          0   \n4      0   Gastritis is an inflammation, irritation, or ...          0   \n\n                                            question  \\\n0                What are the symptoms of gastritis?   \n1     What does the treatment for gastritis involve?   \n2  How does an upper endoscopy help diagnose gast...   \n3  How is a fecal occult blood test (stool test) ...   \n4                                 What is gastritis?   \n\n                                              answer  \n0  However, the most common symptoms include: Nau...  \n1  Treatment for gastritis usually involves: Taki...  \n2  However, the most common symptoms include: Nau...  \n3  This test checks for the presence of blood in ...  \n4   Gastritis is an inflammation, irritation, or ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>context_x</th>\n      <th>context_y</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Gastritis is an inflammation, irritation, or ...</td>\n      <td>0</td>\n      <td>What are the symptoms of gastritis?</td>\n      <td>However, the most common symptoms include: Nau...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Gastritis is an inflammation, irritation, or ...</td>\n      <td>0</td>\n      <td>What does the treatment for gastritis involve?</td>\n      <td>Treatment for gastritis usually involves: Taki...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Gastritis is an inflammation, irritation, or ...</td>\n      <td>0</td>\n      <td>How does an upper endoscopy help diagnose gast...</td>\n      <td>However, the most common symptoms include: Nau...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>Gastritis is an inflammation, irritation, or ...</td>\n      <td>0</td>\n      <td>How is a fecal occult blood test (stool test) ...</td>\n      <td>This test checks for the presence of blood in ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Gastritis is an inflammation, irritation, or ...</td>\n      <td>0</td>\n      <td>What is gastritis?</td>\n      <td>Gastritis is an inflammation, irritation, or ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"context_question = []\nfor index, row in df.iterrows():\n    context_question.append('Question: ' + row['question'] + '\\n\\nAnswer:' +  '\\n\\nContext: ' + row['context_x'])","metadata":{"execution":{"iopub.status.busy":"2023-10-29T02:52:45.543226Z","iopub.execute_input":"2023-10-29T02:52:45.544081Z","iopub.status.idle":"2023-10-29T02:52:47.359509Z","shell.execute_reply.started":"2023-10-29T02:52:45.544047Z","shell.execute_reply":"2023-10-29T02:52:47.358647Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(context_question)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T02:53:24.857617Z","iopub.execute_input":"2023-10-29T02:53:24.857965Z","iopub.status.idle":"2023-10-29T02:53:24.864357Z","shell.execute_reply.started":"2023-10-29T02:53:24.857940Z","shell.execute_reply":"2023-10-29T02:53:24.863436Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"27728"},"metadata":{}}]},{"cell_type":"code","source":"inputs = tokenizer(context_question, \n                   return_tensors=\"pt\", \n                   padding='max_length', \n                   truncation=True,\n                    max_length=2500,\n                  ).to('cuda')\n\nAnswer = df['answer'][:100].tolist()\n\noutputs = tokenizer(Answer, \n                   return_tensors=\"pt\", \n                   padding='max_length', \n                   truncation=True,\n                    max_length=2500,\n                  ).to('cuda')\n\n\ntrain_dataset = TensorDataset(torch.tensor(inputs.input_ids), torch.tensor(outputs.input_ids))","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:48:32.737226Z","iopub.execute_input":"2023-10-29T03:48:32.738062Z","iopub.status.idle":"2023-10-29T03:48:33.211906Z","shell.execute_reply.started":"2023-10-29T03:48:32.738026Z","shell.execute_reply":"2023-10-29T03:48:33.210953Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/55086056.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_dataset = TensorDataset(torch.tensor(inputs.input_ids), torch.tensor(outputs.input_ids))\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs):\n        self.inputs = inputs\n\n    def __len__(self):\n        return len(self.inputs[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {key: self.inputs[key][idx] for key in self.inputs}","metadata":{"execution":{"iopub.status.busy":"2023-10-29T02:55:27.107351Z","iopub.execute_input":"2023-10-29T02:55:27.108127Z","iopub.status.idle":"2023-10-29T02:55:27.114282Z","shell.execute_reply.started":"2023-10-29T02:55:27.108093Z","shell.execute_reply":"2023-10-29T02:55:27.113157Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset = CustomDataset(inputs)\nbatch_size = 1  # Adjust the batch size as needed\ndataloader = DataLoader(dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T02:55:30.793076Z","iopub.execute_input":"2023-10-29T02:55:30.793449Z","iopub.status.idle":"2023-10-29T02:55:30.798726Z","shell.execute_reply.started":"2023-10-29T02:55:30.793419Z","shell.execute_reply":"2023-10-29T02:55:30.797436Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache() ","metadata":{"execution":{"iopub.status.busy":"2023-10-29T02:55:59.690056Z","iopub.execute_input":"2023-10-29T02:55:59.690433Z","iopub.status.idle":"2023-10-29T02:55:59.695016Z","shell.execute_reply.started":"2023-10-29T02:55:59.690400Z","shell.execute_reply":"2023-10-29T02:55:59.693997Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for batch in dataloader:\n    batch = {key: batch[key].to('cuda') for key in batch}\n    with torch.no_grad():\n        outputs = model.generate(**batch).to('cuda')\n#     answers.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n        torch.cuda.empty_cache()  # Clear GPU cache\n\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True, max_length=200))\n# outputs = model.generate(**inputs).to('cuda')\n# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:00:46.497552Z","iopub.execute_input":"2023-10-29T03:00:46.497941Z","iopub.status.idle":"2023-10-29T03:00:46.511802Z","shell.execute_reply.started":"2023-10-29T03:00:46.497911Z","shell.execute_reply":"2023-10-29T03:00:46.510865Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 1024)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 16)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 16)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=1024, bias=False)\n              (k): Linear(in_features=1024, out_features=1024, bias=False)\n              (v): Linear(in_features=1024, out_features=1024, bias=False)\n              (o): Linear(in_features=1024, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/peft.git","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:04:40.605203Z","iopub.execute_input":"2023-10-29T03:04:40.605965Z","iopub.status.idle":"2023-10-29T03:05:09.349527Z","shell.execute_reply.started":"2023-10-29T03:04:40.605929Z","shell.execute_reply":"2023-10-29T03:05:09.348587Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting git+https://github.com/huggingface/peft.git\n  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-_9b3kwc3\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-_9b3kwc3\n  Resolved https://github.com/huggingface/peft.git to commit 207229ad5e1854cd897306ce5ee725b0245b1064\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (4.33.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (0.22.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.6.0.dev0) (0.3.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.6.0.dev0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.6.0.dev0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.6.0.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.6.0.dev0) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.6.0.dev0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.6.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.6.0.dev0) (0.13.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->peft==0.6.0.dev0) (2023.9.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.6.0.dev0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.6.0.dev0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.6.0.dev0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.6.0.dev0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.6.0.dev0) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.6.0.dev0) (1.3.0)\nBuilding wheels for collected packages: peft\n  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for peft: filename=peft-0.6.0.dev0-py3-none-any.whl size=124489 sha256=af694246c6df624c0bce4fc160f3a01cb74e155846256a26cba705a0a136c6f0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xj6n2d93/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\nSuccessfully built peft\nInstalling collected packages: peft\nSuccessfully installed peft-0.6.0.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:06:14.387666Z","iopub.execute_input":"2023-10-29T03:06:14.388085Z","iopub.status.idle":"2023-10-29T03:06:14.434943Z","shell.execute_reply.started":"2023-10-29T03:06:14.388052Z","shell.execute_reply":"2023-10-29T03:06:14.433954Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"config = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\n        \"q\",\n        \"k\",\n        \"v\",\n        \"o\",\n        \"wi_0\",\n        \"wi_1\",\n        \"wo\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:12:18.731749Z","iopub.execute_input":"2023-10-29T03:12:18.732121Z","iopub.status.idle":"2023-10-29T03:12:18.738028Z","shell.execute_reply.started":"2023-10-29T03:12:18.732094Z","shell.execute_reply":"2023-10-29T03:12:18.736799Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model=get_peft_model(model, config)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:12:28.849922Z","iopub.execute_input":"2023-10-29T03:12:28.850580Z","iopub.status.idle":"2023-10-29T03:12:29.607726Z","shell.execute_reply.started":"2023-10-29T03:12:28.850539Z","shell.execute_reply":"2023-10-29T03:12:29.606829Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:12:39.969598Z","iopub.execute_input":"2023-10-29T03:12:39.969998Z","iopub.status.idle":"2023-10-29T03:12:40.012521Z","shell.execute_reply.started":"2023-10-29T03:12:39.969966Z","shell.execute_reply":"2023-10-29T03:12:40.011576Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PeftModelForCausalLM(\n      (base_model): LoraModel(\n        (model): T5ForConditionalGeneration(\n          (shared): Embedding(32128, 1024)\n          (encoder): T5Stack(\n            (embed_tokens): Embedding(32128, 1024)\n            (block): ModuleList(\n              (0): T5Block(\n                (layer): ModuleList(\n                  (0): T5LayerSelfAttention(\n                    (SelfAttention): T5Attention(\n                      (q): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (k): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (v): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (o): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (relative_attention_bias): Embedding(32, 16)\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                  (1): T5LayerFF(\n                    (DenseReluDense): T5DenseGatedActDense(\n                      (wi_0): Linear(\n                        in_features=1024, out_features=2816, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=2816, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (wi_1): Linear(\n                        in_features=1024, out_features=2816, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=2816, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (wo): Linear(\n                        in_features=2816, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=2816, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (dropout): Dropout(p=0.1, inplace=False)\n                      (act): NewGELUActivation()\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                )\n              )\n              (1-23): 23 x T5Block(\n                (layer): ModuleList(\n                  (0): T5LayerSelfAttention(\n                    (SelfAttention): T5Attention(\n                      (q): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (k): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (v): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (o): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                  (1): T5LayerFF(\n                    (DenseReluDense): T5DenseGatedActDense(\n                      (wi_0): Linear(\n                        in_features=1024, out_features=2816, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=2816, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (wi_1): Linear(\n                        in_features=1024, out_features=2816, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=2816, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (wo): Linear(\n                        in_features=2816, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=2816, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (dropout): Dropout(p=0.1, inplace=False)\n                      (act): NewGELUActivation()\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                )\n              )\n            )\n            (final_layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (decoder): T5Stack(\n            (embed_tokens): Embedding(32128, 1024)\n            (block): ModuleList(\n              (0): T5Block(\n                (layer): ModuleList(\n                  (0): T5LayerSelfAttention(\n                    (SelfAttention): T5Attention(\n                      (q): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (k): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (v): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (o): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (relative_attention_bias): Embedding(32, 16)\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                  (1): T5LayerCrossAttention(\n                    (EncDecAttention): T5Attention(\n                      (q): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (k): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (v): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (o): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                  (2): T5LayerFF(\n                    (DenseReluDense): T5DenseGatedActDense(\n                      (wi_0): Linear(\n                        in_features=1024, out_features=2816, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=2816, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (wi_1): Linear(\n                        in_features=1024, out_features=2816, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=2816, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (wo): Linear(\n                        in_features=2816, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=2816, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (dropout): Dropout(p=0.1, inplace=False)\n                      (act): NewGELUActivation()\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                )\n              )\n              (1-23): 23 x T5Block(\n                (layer): ModuleList(\n                  (0): T5LayerSelfAttention(\n                    (SelfAttention): T5Attention(\n                      (q): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (k): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (v): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (o): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                  (1): T5LayerCrossAttention(\n                    (EncDecAttention): T5Attention(\n                      (q): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (k): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (v): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (o): Linear(\n                        in_features=1024, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                  (2): T5LayerFF(\n                    (DenseReluDense): T5DenseGatedActDense(\n                      (wi_0): Linear(\n                        in_features=1024, out_features=2816, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=2816, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (wi_1): Linear(\n                        in_features=1024, out_features=2816, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=1024, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=2816, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (wo): Linear(\n                        in_features=2816, out_features=1024, bias=False\n                        (lora_dropout): ModuleDict(\n                          (default): Dropout(p=0.05, inplace=False)\n                        )\n                        (lora_A): ModuleDict(\n                          (default): Linear(in_features=2816, out_features=32, bias=False)\n                        )\n                        (lora_B): ModuleDict(\n                          (default): Linear(in_features=32, out_features=1024, bias=False)\n                        )\n                        (lora_embedding_A): ParameterDict()\n                        (lora_embedding_B): ParameterDict()\n                      )\n                      (dropout): Dropout(p=0.1, inplace=False)\n                      (act): NewGELUActivation()\n                    )\n                    (layer_norm): T5LayerNorm()\n                    (dropout): Dropout(p=0.1, inplace=False)\n                  )\n                )\n              )\n            )\n            (final_layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (lm_head): Linear(\n            in_features=1024, out_features=32128, bias=False\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=1024, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=32128, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n        )\n      )\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.device_count() > 1:\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:16:29.594159Z","iopub.execute_input":"2023-10-29T03:16:29.594921Z","iopub.status.idle":"2023-10-29T03:16:29.599706Z","shell.execute_reply.started":"2023-10-29T03:16:29.594886Z","shell.execute_reply":"2023-10-29T03:16:29.598687Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom datetime import datetime\n\nproject = \"journal-finetune\"\nbase_model_name = \"flan_T5\"\nrun_name = base_model_name + '-' + project\noutput_dir = \"./\"+run_name\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset = train_dataset,\n    #data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    args = transformers.TrainingArguments(\n        output_dir=output_dir,\n        warmup_steps=1,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=1,\n        max_steps=500,\n        learning_rate=2.5e-5,\n        optim=\"adafactor\",\n        logging_steps=25,\n        evaluation_strategy=\"steps\",\n        save_steps=25\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T04:12:32.175300Z","iopub.execute_input":"2023-10-29T04:12:32.176075Z","iopub.status.idle":"2023-10-29T04:12:32.192719Z","shell.execute_reply.started":"2023-10-29T04:12:32.176041Z","shell.execute_reply":"2023-10-29T04:12:32.191607Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"!pip install bnb","metadata":{"execution":{"iopub.status.busy":"2023-10-29T03:59:52.184455Z","iopub.execute_input":"2023-10-29T03:59:52.185228Z","iopub.status.idle":"2023-10-29T04:00:08.911124Z","shell.execute_reply.started":"2023-10-29T03:59:52.185187Z","shell.execute_reply":"2023-10-29T04:00:08.910112Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting bnb\n  Using cached bnb-0.3.0-py3-none-any.whl (9.7 kB)\nCollecting attrs<21.0.0,>=20.2.0 (from bnb)\n  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\nCollecting click<8.0.0,>=7.1.2 (from bnb)\n  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\nRequirement already satisfied: markdown<4.0.0,>=3.3.3 in /opt/conda/lib/python3.10/site-packages (from bnb) (3.4.3)\nCollecting pyyaml<6.0.0,>=5.3.1 (from bnb)\n  Using cached PyYAML-5.4.1.tar.gz (175 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[62 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m /tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n  \u001b[31m   \u001b[0m !!\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m         ********************************************************************************\n  \u001b[31m   \u001b[0m         The license_file parameter is deprecated, use license_files instead.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m         By 2023-Oct-30, you need to update your project and remove deprecated calls\n  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m         See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n  \u001b[31m   \u001b[0m         ********************************************************************************\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m !!\n  \u001b[31m   \u001b[0m   parsed = self.parsers.get(option_name, lambda x: x)(value)\n  \u001b[31m   \u001b[0m running egg_info\n  \u001b[31m   \u001b[0m writing lib3/PyYAML.egg-info/PKG-INFO\n  \u001b[31m   \u001b[0m writing dependency_links to lib3/PyYAML.egg-info/dependency_links.txt\n  \u001b[31m   \u001b[0m writing top-level names to lib3/PyYAML.egg-info/top_level.txt\n  \u001b[31m   \u001b[0m Traceback (most recent call last):\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n  \u001b[31m   \u001b[0m     main()\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n  \u001b[31m   \u001b[0m     return hook(config_settings)\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 355, in get_requires_for_build_wheel\n  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 325, in _get_build_requires\n  \u001b[31m   \u001b[0m     self.run_setup()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 341, in run_setup\n  \u001b[31m   \u001b[0m     exec(code, locals())\n  \u001b[31m   \u001b[0m   File \"<string>\", line 271, in <module>\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/__init__.py\", line 103, in setup\n  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n  \u001b[31m   \u001b[0m     return run_commands(dist)\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n  \u001b[31m   \u001b[0m     dist.run_commands()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n  \u001b[31m   \u001b[0m     self.run_command(cmd)\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/dist.py\", line 989, in run_command\n  \u001b[31m   \u001b[0m     super().run_command(command)\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n  \u001b[31m   \u001b[0m     cmd_obj.run()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 318, in run\n  \u001b[31m   \u001b[0m     self.find_sources()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 326, in find_sources\n  \u001b[31m   \u001b[0m     mm.run()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 548, in run\n  \u001b[31m   \u001b[0m     self.add_defaults()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 586, in add_defaults\n  \u001b[31m   \u001b[0m     sdist.add_defaults(self)\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/command/sdist.py\", line 113, in add_defaults\n  \u001b[31m   \u001b[0m     super().add_defaults()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/_distutils/command/sdist.py\", line 251, in add_defaults\n  \u001b[31m   \u001b[0m     self._add_defaults_ext()\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/_distutils/command/sdist.py\", line 336, in _add_defaults_ext\n  \u001b[31m   \u001b[0m     self.filelist.extend(build_ext.get_source_files())\n  \u001b[31m   \u001b[0m   File \"<string>\", line 201, in get_source_files\n  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-7mfmsbxc/overlay/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 107, in __getattr__\n  \u001b[31m   \u001b[0m     raise AttributeError(attr)\n  \u001b[31m   \u001b[0m AttributeError: cython_sources\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n\n\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T04:12:35.201761Z","iopub.execute_input":"2023-10-29T04:12:35.202140Z","iopub.status.idle":"2023-10-29T04:12:36.733185Z","shell.execute_reply.started":"2023-10-29T04:12:35.202109Z","shell.execute_reply":"2023-10-29T04:12:36.731704Z"},"trusted":true},"execution_count":59,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1813\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1813\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1814\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_utils.py:707\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    706\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:109\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 109\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mvars\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m    110\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    111\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 109\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m    110\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    111\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n","\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"],"ename":"TypeError","evalue":"vars() argument must have __dict__ attribute","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}